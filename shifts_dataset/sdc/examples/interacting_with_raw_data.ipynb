{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ysdc_dataset_api.utils import get_file_paths, scenes_generator, transform_2d_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load protobufs for training dataset\n",
    "dataset_path = '/path/to/dataset/dir'\n",
    "filepaths = get_file_paths(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one scene\n",
    "scene = next(scenes_generator(filepaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of history steps: 25\n"
     ]
    }
   ],
   "source": [
    "# Number of known history steps\n",
    "# Index 0 is farthest (-5s) into the past, and index 24 represents current time\n",
    "print('Number of history steps:', len(scene.past_vehicle_tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vehicles seen at step 24: 30\n",
      "Params of the vehicle with index 0\n",
      "track_id: 263\n",
      "position {\n",
      "  x: 110.12437852357087\n",
      "  y: 382.6800355662276\n",
      "  z: 0.8349941968917847\n",
      "}\n",
      "dimensions {\n",
      "  x: 2.0618557929992676\n",
      "  y: 2.0618557929992676\n",
      "  z: 1.6699883937835693\n",
      "}\n",
      "linear_velocity {\n",
      "}\n",
      "linear_acceleration {\n",
      "  x: -0.1384161281761373\n",
      "  y: -0.08170445984182287\n",
      "}\n",
      "yaw: 1.3430629801242444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_step = 24\n",
    "vehicle_ind = 0\n",
    "\n",
    "print(f'Number of vehicles seen at step {history_step}:', len(scene.past_vehicle_tracks[-1].tracks))\n",
    "print(f'Params of the vehicle with index {vehicle_ind}')\n",
    "print(scene.past_vehicle_tracks[history_step].tracks[vehicle_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pedestrians seen at step 24: 0\n"
     ]
    }
   ],
   "source": [
    "pedestrian_ind = 0\n",
    "n_pedestrians = len(scene.past_pedestrian_tracks[-1].tracks)\n",
    "\n",
    "print(f'Number of pedestrians seen at step {history_step}: {n_pedestrians}')\n",
    "if n_pedestrians > 0:\n",
    "    print(scene.past_pedestrian_tracks[history_step].tracks[pedestrian_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vehicles to predict: 15\n",
      "\n",
      "track_id: 209\n",
      "trajectory_tags: kStationary\n",
      "trajectory_tags: kUniform\n",
      "\n",
      "track_id: 224\n",
      "trajectory_tags: kUniform\n",
      "\n",
      "track_id: 225\n",
      "trajectory_tags: kStationary\n",
      "trajectory_tags: kUniform\n",
      "trajectory_tags: kAcceleration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Number of vehicles to predict:', len(scene.prediction_requests))\n",
    "print()\n",
    "for prediction_request in scene.prediction_requests[:3]:\n",
    "    print(prediction_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of future steps to predict: 25\n",
      "First future state:\n",
      "track_id: 259\n",
      "position {\n",
      "  x: 83.92500636022204\n",
      "  y: 321.4359659211941\n",
      "  z: 0.9676238953835\n",
      "}\n",
      "dimensions {\n",
      "  x: 4.98116276951867\n",
      "  y: 2.181139889527\n",
      "  z: 1.935247790767\n",
      "}\n",
      "linear_velocity {\n",
      "  x: 0.22496777564149872\n",
      "  y: -0.214686082882927\n",
      "}\n",
      "linear_acceleration {\n",
      "  x: 0.596252911933497\n",
      "  y: -0.7376466017305991\n",
      "}\n",
      "yaw: 2.7768723056898876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of future steps\n",
    "# Index 0 is closest (0.2s into the future), index 24 is farthest (5s into the future)\n",
    "print('Number of future steps to predict:', len(scene.future_vehicle_tracks))\n",
    "print('First future state:')\n",
    "print(scene.future_vehicle_tracks[0].tracks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52fd37fc19794e402c13085b7708ca3d356bc0f59c68efac4f5e72fab90944f9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.pytorch': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
